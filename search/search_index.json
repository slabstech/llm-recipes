{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"LLM Recipes","text":""},{"location":"index.html#introduction","title":"Introduction","text":"<p>LLM Recipes is a collection of projects and tools for experimental usage of LLM</p>"},{"location":"index.html#projects","title":"Projects","text":"<ul> <li>Bhoomi</li> <li>bhoomi-docs</li> </ul> Concept Status Tech NoteBook LLama Complete Python + TTS Quantisation Complete llama.cpp Image/Scene Recognition + v0.3 Complete llava/moondream Text Query + API Calls Complete mistral7B-v0.3 + ollama + RestAPI"},{"location":"index.html#tutorials","title":"Tutorials","text":"<ul> <li>Tutorials</li> </ul>"},{"location":"index.html#extra","title":"Extra","text":"<ul> <li>Clean Install of Ubuntu + Docker + Nvidia Requirements</li> </ul>"},{"location":"index.html#upcoming-challenges","title":"Upcoming Challenges","text":"<ul> <li>Hackathons</li> </ul>"},{"location":"index.html#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Thanks to the contributors and maintainers of the third-party libraries used in this project.</li> </ul>"},{"location":"index.html#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"chat-rtx.html","title":"Chat rtx","text":"<p>Chat with RTX</p> <p>Run mistral + RAG </p>"},{"location":"clean-ubuntu-setup.html","title":"Install Everything for llm-recpies from scratch","text":"<ul> <li> <p>Ubuntu Library setup</p> <ul> <li>After Ubuntu is installed, update all required libraries<ul> <li>sudo apt-get update</li> <li>sudo apt-get upgrade</li> <li>sudo apt dist-upgrade</li> <li>sudo apt upgrade</li> <li>sudo apt install git</li> </ul> </li> <li>Download VScode from link<ul> <li>cd Downloads &amp;&amp; sudo dpkg -i code_*.deb</li> </ul> </li> <li>Install Docker  - Install &amp; Post-install <ul> <li>sudo apt-get update</li> <li>sudo apt-get install ca-certificates curl</li> <li>sudo install -m 0755 -d /etc/apt/keyrings</li> <li>sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc</li> <li>sudo chmod a+r /etc/apt/keyrings/docker.asc</li> <li><code>echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\</code></li> <li>sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</li> <li>sudo apt-get update</li> <li>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</li> <li>sudo docker run hello-world</li> <li>sudo groupadd docker</li> <li>sudo usermod -aG docker $USER</li> <li>newgrp docker</li> <li>docker run hello-world</li> </ul> </li> <li>Docker Desktop - Optional<ul> <li>sudo apt install gnome-terminal</li> <li>Download the deb package</li> <li>cd Downloads &amp;&amp; sudo dpkg -i docker-desktop*.deb</li> <li>sudo apt-get install -f</li> </ul> </li> <li>Setup ssh authentication to GitHub Step 1 &amp; Step 2 </li> </ul> </li> <li> <p>NVIDIA - Driver setup</p> <ul> <li>nvidia container toolkit install<ul> <li><code>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list</code></li> <li>sudo apt-get update</li> <li>sudo apt-get install -y nvidia-container-toolkit</li> </ul> </li> <li>Configure docker<ul> <li>sudo nvidia-ctk runtime configure --runtime=docker</li> <li>systemctl --user restart docker</li> <li>nvidia-ctk runtime configure --runtime=docker --config=$HOME/.config/docker/daemon.json</li> <li>systemctl --user restart docker</li> <li>sudo nvidia-ctk config --set nvidia-container-cli.no-cgroups --in-place</li> </ul> </li> <li>Install cuda &amp; cuda-driver &amp; wget downloads . Note - Below versions are applicable as of March 30, 2024. You will have different version later. This is only a guide of steps.<ul> <li>sudo apt install build-essential</li> <li>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</li> <li>sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600</li> <li>wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.deb</li> <li>sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.deb</li> <li>sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/</li> <li>sudo apt-get update</li> <li>sudo apt-get -y install cuda-toolkit-12-4</li> <li>sudo apt-get install cuda-toolkit</li> <li>sudo reboot</li> </ul> </li> </ul> </li> <li> <p>llm-recipes library</p> <ul> <li>git clone https://github.com/slabstech/llm-recipes</li> </ul> </li> <li> <p>https://jupyter.org/install : pip install notebook/ jupyter notebook</p> </li> </ul>"},{"location":"code-pair.html","title":"Code pair","text":"<p>Code - Pair programmer</p> <ul> <li> <p>vscode + continue + ollama + mistral-7B</p> </li> <li> <p>Use the config.json file from src/continue-dev for 'continue' extension. It will use the ollama + mistral + docker setup.</p> </li> <li> <p>Reference</p> <ul> <li>https://medium.com/@omargohan/using-ollama-in-your-ide-with-continue-e8cefeeee033</li> <li>https://continue.dev/</li> <li>https://vscodium.com/</li> <li>https://www.ycombinator.com/companies/continue/jobs</li> </ul> </li> </ul>"},{"location":"data-fusion.html","title":"Data fusion","text":"<p>Sensor Data Fusion</p> <p>Use output of vision model to get more information with llm</p> <p>How can we generate info and knowledge from images.</p> <p>Feed the output as prompt, see how it augments the results</p> <p>Instruction following for Vision models with Action</p> <p>Example - Watch a football match with comments and get insights, strategy creation from old videos</p> <p>Augment robots with dynamic instruction following. Read sign boards and give instructions to robot.</p>"},{"location":"demo-speech-to-speech-inference.html","title":"Speech to Speech inference","text":"<p>Hello, Everyone.  My project is llm-recipes. </p> <p>The project page is at slabstech.com/llm-recipes and setup details with code is available at github.com/slabstech/llm-recipes.</p> <p>I am happy to present my experiments with Generative AI with focus on local/on-premise deployment for data security. </p> <p>We will demo an Alexa/Siri like service with end to end processing done locally. We will showcase Speech to Speech Inference.</p>"},{"location":"demo-speech-to-speech-inference.html#system-diagram","title":"System Diagram","text":"<p>With Open-weights release of LLM models with persmissive license,  models capable of GPT-3 level intelligence can deployed on a personal computer. GPT3.5 level model's are available but require server/workstation with multiple GPU for inference.</p> <p>The program flow can be split into 3 modules. 1. Automatic Speech recognition using whisper  2. Natural language query based API request using Mistral7b-v3 with function calling  3. Text to Speech using XYZ to provide the result in voice format.</p> <p>The demo is narrowed to PetStore query and can be extended to other data stores. </p> <p>We plan to further optimize the project to run with lower latency. For better speeds, it is suggested to run on a multi-gpu system, with each module running in a dedicated GPU card for inference. This project would require minimum 3 GPU's .</p>"},{"location":"demo-speech-to-speech-inference.html#system-configuration","title":"System Configuration","text":"<p>We have experimented the project with the following configuration for deployment testing, we have not made any bechmarks yet due to time constraints. </p> <ol> <li>Laptop with 16GB RAM , no gpu</li> <li>Laptop with 32GB RAM, no gpu</li> <li>Laptop wirh 32GB RAM, 12 GB gpu</li> <li>Workstation with xx RAM, 1x 24GB gpu</li> <li>Workstation with xx RAM, 3x 24GB gpu </li> </ol> <p>The project experiments are open sourced, you're encouraged to run the project on your systems. You can raise an issue on the GitHub project for any problems and queries. Also we would be happy to accept contributions to the code,documentation and feature request. </p>"},{"location":"deployment.html","title":"Deployment","text":"<p>Deployment </p> <p>Docker compose with two ollama containers. Using the same volume for data/image reuse</p> <ul> <li>Experiment </li> <li>multiome model load with single container - MAX_NUM_MODELS</li> <li> <p>Use two containers for mistral7b and moondream</p> </li> <li> <p>Write a script which analyse memory availability through nvidia-smi and starts containers as accordingly</p> </li> </ul> <p>--  Single container - Dockerfiles 1. Cpu only 2. Gpu only </p> <p>Monolithic build  Call internal libs directly </p> <p>Full control of all components,  Optimized for usability, </p> <p>Do performance optimization later after demo</p> <p>Build  1.whisper  2. Coqui 3. Ollama  4. Ffmpeg </p> <p>Download mistrak7b, whisper base , coqui - tachtron, voice clone </p> <p>Multi stage docker build</p> <p>Build individual components and finally copy only build libraries to deploy image </p> <p>Use cpu and gpu separately  -  Image name - speech-to-speech-cpu</p> <p>first cpu only, push to docker hub for docker compose .</p> <p>Multi container - Modify existing compose files for multi container support with gpu. Use the prebuilt binaries for gpu. Added additonal data after download and rebuild docker file.  Save a checkpoint for immediate use.</p> <ul> <li>Reference<ul> <li>Docker - Whisper X<ul> <li>https://github.com/jim60105/docker-whisperX</li> <li>https://github.com/ahmetoner/whisper-asr-webservice/blob/main/docker-compose.gpu.yml</li> <li>https://github.com/jim60105/docker-whisperX/blob/master/Dockerfile</li> </ul> </li> </ul> </li> </ul>"},{"location":"dspy.html","title":"Dspy","text":"<p>DSPy</p> <p>pip install dspy-ai</p> <p>Reference - https://dspy-docs.vercel.app/docs/cheatsheet - https://github.com/stanfordnlp/dspy - https://dspy-docs.vercel.app/ - Ollama + Mistral : https://gist.github.com/jrknox1977/78c17e492b5a75ee5bbaf9673aee4641 - https://github.com/weaviate/recipes/tree/main/integrations/dspy - https://dspy-docs.vercel.app/docs/tutorials/rag - https://dspy-docs.vercel.app/api/local_language_model_clients/Ollama</p>"},{"location":"events.html","title":"Events","text":"<p>Events</p> <ul> <li>Baiosphere - https://baiosphere.org/en/ai-events/?time=month&amp;yr=2024&amp;dy=1&amp;month=8</li> </ul>"},{"location":"function_call_rest_api.html","title":"Function call rest api","text":"<p>Function Calling with REST API</p> <p>Get real time info by fetching details from REST API</p> <ul> <li>Create tools definition dynamically by parsing json from SwaggerUI</li> <li>Pass tools definition array to ChatCompletionRequest</li> <li>Create list of queries - LLM model for problem statement</li> <li>Pass queries to Model request as array.</li> <li> <p>Write a parser for token output to get function calling params </p> </li> <li> <p>Create AutoGen workflow to automate the above steps</p> </li> <li> <p>Future</p> <ul> <li>With stable diffusion, create dashboards with interactive graphics ?</li> </ul> </li> </ul>"},{"location":"genUI.html","title":"genUI","text":"<p>Gen UI</p> <ul> <li>Demo  widget creator based on OpenAPI spec backend API</li> <li>API function autogenerated by SwaggerUI</li> <li>Updated current Swagger UI definition to make it compatible with OpenAPI 3.0 spec</li> <li> <p>Test user messsage in different language</p> </li> <li> <p>Create template and code functionality using Mistral</p> </li> <li>Create prompt for UI widgets from mistral and select required UI generated with StableDiffusion</li> <li>Convert the images to HTML template using LLAVA</li> <li> <p>Connect different widgets with JS code </p> </li> <li> <p>Automate the workflow using AutoGen</p> </li> <li> <p>Code</p> <ul> <li>make function calling reliable</li> <li>experiment with tools/messages/results</li> <li>improve docs for cookbook<ul> <li>update cookbook with tool creator</li> </ul> </li> </ul> </li> <li> <p>Run finetuning with</p> <ul> <li>Example dataset</li> <li>gaganyatri pdf</li> <li>all books </li> <li>slabstech code</li> </ul> </li> <li> <p>Learn how to quantize model to 4-bit</p> </li> </ul>"},{"location":"gpus.html","title":"Gpus","text":"<p>GPUS's</p> <ul> <li>Fly.io</li> <li>https://fly.io/docs/gpus/python-gpu-example/</li> <li>https://fly.io/docs/gpus/</li> <li>Repos<ul> <li>https://github.com/superfly/</li> </ul> </li> <li> <p>Whisper Demo</p> <ul> <li>https://github.com/fly-apps/cog-whisper</li> </ul> </li> <li> <p>External Dock for GPU</p> </li> <li> <p>https://www.amazon.de/dp/B0DCZF77VT</p> </li> <li> <p>Minisforum MS-01  - MiniPC - MS-A1 ryzen powered minin PC</p> </li> <li>Corsair - 850 W : https://www.amazon.com/dp/B0DJ1JL3MK?th=1</li> </ul>"},{"location":"hackathons.html","title":"Hackathons","text":"<p>Hackathons</p> <ul> <li>Current - <ul> <li>Eleven Labs - 22-23 Feb</li> <li>ArangoDB - &lt; 8 March 2025</li> <li>Predictive HEalthcare - &lt; 1 March 2025</li> <li>Snapdragon AI - &lt; 25 FEb 2025</li> </ul> </li> <li> <p>Upcoming</p> </li> <li> <p>Past</p> <ul> <li>ETHGlobal - Ethereum Agents Jan 30-Feb 12</li> <li>EDF Munich - 2025 Feb 13-16</li> <li>hackaTUM - Nov 22-24</li> <li>Assembly AI - Nov 13-24</li> <li>Ollama + PG AI Oct 30- Nov 10</li> <li>Sahamati - Oct 4 - Nov 4 - https://sahamati-buildaathon-2024.devfolio.co/ devfolio- Account Aggreation</li> <li>RedHat-Intel &lt; Nov 4</li> <li>Wanddb - HAckerCup - Oct 16</li> <li>Nvidia Hack AI -Oct 3</li> <li>Valorant + AWS - Oct 12details</li> </ul> </li> <li> <p>HackNow 2024 - July, 2024 -     </p> <ul> <li>Supermarket Analytics for Total Food Waste Elimination</li> </ul> </li> <li>Europe Defense Tech - June 28-29, 2024 <ul> <li>Build Real time Vision model for Security</li> </ul> </li> <li>Hackathon - May 25-26, 2024 - Real time - browsing Capacity  <ul> <li>Create function with Rest api endpoint detection </li> </ul> </li> </ul>"},{"location":"hackathons.html#-edth-amsterdam-march-28-30","title":"- EDTH - Amsterdam March 28-30","text":""},{"location":"ideas.html","title":"Ideas","text":"<p>Ideas </p> <ul> <li>Inspiration to Emulate<ul> <li>Demo's of LLM for Everyday use Run Locally - Ideas to implement    <ul> <li>Alexa/Siri/Google Voice Commands - Example</li> <li>Figure 01 Robot Demo- Example</li> <li>Ipad Calculator App - Example</li> </ul> </li> </ul> </li> </ul>"},{"location":"llama-cpp.html","title":"Llama cpp","text":"<p>Raspi Module</p> <p>Installation steps  - sudo apt update &amp;&amp; sudo apt install git - sudo apt-get install git-lfs - git lfs install - mkdir piRun - cd piRun - python -m venv env - source env/bin/activate</p> <ul> <li>python3 -m pip install torch numpy sentencepiece</li> <li> <p>sudo apt install g++ build-essential</p> </li> <li> <p>wget https://github.com/ggerganov/llama.cpp/archive/refs/heads/gg/phi-2.zip</p> </li> <li>unzip phi-2.zip </li> <li> <p>rm phi-2.zip</p> </li> <li> <p>cd llama.cpp-gg-phi-2/</p> </li> <li> <p>make </p> </li> <li> <p>mkdir phi-2-gguf/</p> </li> <li>pip install -U huggingface_hub</li> <li>huggingface-cli download TheBloke/phi-2-GGUF --local-dir phi-2-gguf/  </li> </ul> <p>./main -m phi-2-gguf/phi-2.Q4_K_M.gguf -p \"Question: Write a python function to print the first n numbers in the fibonacci series\"</p> <p>Alternate Steps for gguf model build from source</p> <ul> <li>Download - https://huggingface.co/microsoft/phi-2</li> <li>pip install -U huggingface_hub</li> <li> <p>huggingface-cli download microsoft/phi-2</p> </li> <li> <p>python convert-hf-to-gguf.py phi-2</p> </li> </ul> <p>./main -m phi-2/ggml-model-f16.gguf -p \"Question: Write a python function to print the first n numbers in the fibonacci series\"</p> <p>-- model Deployment</p> <p>./main -m models/phi-2.Q4_0.gguf -p \"Question: Write a python function to print the first n numbers in the fibonacci series\"</p> <p>Reference - https://www.dfrobot.com/blog-13498.html</p> <ul> <li>Docker on Raspi - https://docs.docker.com/engine/install/debian/</li> <li>https://huggingface.co/TheBloke/phi-2-GGUF</li> <li>https://ubuntu.com/blog/deploying-open-language-models-on-ubuntu</li> </ul>"},{"location":"misc.html","title":"Misc","text":"<p>Challenges</p> <p>https://content.neuralink.com/compression-challenge/README.html</p>"},{"location":"ollama-open-webui.html","title":"Ollama open webui","text":"<p>v1 = Build a ChatGPT like setup on local machine with Ollama + Open-WebUI + Mistral + Docker</p> <ul> <li> <p>Requirements</p> <ul> <li>CUDA installation - Needs NVIDIA Graphics Card</li> <li>Docker Setup / Docker Desktop on Windows</li> <li>VSCode Terminal</li> </ul> </li> <li> <p>Steps - (VSCode Terminal)</p> <ul> <li>Clone Repo - https://github.com/slabstech/llm-recipes<ul> <li><code>git clone https://github.com/slabstech/llm-recipes.git</code></li> <li><code>cd llm-recipes</code></li> </ul> </li> <li>From src/ollama directory, run the command<ul> <li><code>cd src/ollama</code></li> <li><code>docker-compose.exe -f .\\docker-compose.yml up -d</code></li> <li>Verify if ollama backend is running at http://localhost:11434</li> </ul> </li> <li>Sign Up with any credentials at http://localhost:3000/auth/</li> <li></li> <li> <p>To make UI accessible across local network</p> <ul> <li>Windows<ul> <li>Enable Network Sharing</li> <li>Create Firewall rule with command on Windows PowerShell<ul> <li><code>New-NetFirewallRule -DisplayName \"UI for LLM\" -Direction Inbound -Protocol TCP -LocalPort 3000 -Action Allow</code></li> </ul> </li> <li>Get IP address from CMD shell<ul> <li><code>ipconfig</code></li> <li>Check for IPv4 Address for WIFI/LAN(Ethernet Adaptor)</li> </ul> </li> <li>Access from Mobile/Laptop Brower fron the local network<ul> <li>&lt; YourIPAddress &gt;:3000</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>References</p> <ul> <li>Download models and UI <ul> <li>https://www.youtube.com/watch?v=syR0fT0rkgY&amp;t=314s</li> </ul> </li> <li>https://medium.com/@omargohan/using-ollama-in-your-ide-with-continue-e8cefeeee033</li> <li>https://github.com/open-webui/open-webui</li> <li>https://github.com/ollama</li> <li>mistral.ai</li> </ul> </li> </ul> <p>TODO - </p> <ul> <li>Tutorial- LLM RECIPES </li> </ul> <p>Provide link for Docker and Cuda installation. </p> <p>Update steps for linux / GPU server </p> <ul> <li>open step</li> <li>how to build hardware and system build </li> </ul> <p>Function calling and RAG with RLHF</p>"},{"location":"persona.html","title":"Persona","text":"<p>Personal - Digital Twin</p> <p>An attempt to build a Digital twin of your Persona.</p> <ul> <li> <p>Idea </p> <ul> <li>Generate dataset of all Online activity and notes</li> <li>Create tokens of your dataset</li> <li>FineTune/Post-training of LLM models using LoRA/RAG</li> <li>Create an end-point with your own Persona</li> </ul> </li> <li> <p>Steps</p> <ul> <li>Convert all your markdown to pdf<ul> <li>https://slabstech.com/books/ or https://github.com/slabstech/slabstech.github.io/tree/main/assets/pdf </li> </ul> </li> <li>Create tokens using tiktoken or minbpe<ul> <li>https://github.com/openai/tiktoken</li> <li>https://github.com/karpathy/minbpe </li> </ul> </li> <li>Finetune / post-training of LLM<ul> <li>https://docs.runpod.io/tutorials/pods/fine-tune-llm-axolotl</li> </ul> </li> </ul> </li> <li> <p>Work</p> <ul> <li>pip install requests</li> <li>pip install tiktoken</li> <li>pip install --upgrade openai</li> <li>pip install mistral-common    </li> </ul> </li> </ul>"},{"location":"quantization.html","title":"Quantization","text":"<p>Quantization</p> <p>Reference - https://kaitchup.substack.com/p/mistral-7b-recipes-for-fine-tuning - https://colab.research.google.com/drive/17zT5sLs_f3M404OWhEcwtnlmMKFz3FM7?usp=sharing</p>"},{"location":"remote-jupyter-notebook.html","title":"Remote jupyter notebook","text":"<p>Access Jupyter notebook from remote server</p> <ul> <li> <p>On Remote server</p> <ul> <li>Create python  virtual environment<ul> <li>python -m venv venv</li> </ul> </li> <li>Enable virtual environment<ul> <li>source -m venv venv</li> </ul> </li> <li>Install Jupyter notebook<ul> <li>pip install notebook</li> </ul> </li> <li>Enable security for jupyter server<ul> <li>jupyter server password<ul> <li>Enter new password</li> </ul> </li> </ul> </li> <li>Start notebook <ul> <li>jupyter notebook --no-browser --port=8888</li> </ul> </li> </ul> </li> <li> <p>On Local Machine</p> <ul> <li> <p>ssh -L 8888:localhost:8888 remote-user@remote-server</p> </li> <li> <p>vistit localhost:8888 on the broweser</p> </li> </ul> </li> </ul>"},{"location":"space.html","title":"Space","text":"<p>Analytics for Space</p> <p>https://learn.microsoft.com/en-us/training/paths/introduction-python-space-exploration-nasa/</p> <p>https://github.com/sachinsshetty/passpredict</p>"},{"location":"text-to-speech.html","title":"Text to speech","text":"<p>Text to Speech</p> <ul> <li> <ul> <li>Clone Repo</li> </ul> </li> <li>git clone https://github.com/coqui-ai/TTS.git</li> <li>Setup virtual environment</li> <li>python -m venv venv</li> <li>source venv/bin/activate</li> <li> <p>Install TTS library</p> <ul> <li>pip install TTS</li> </ul> </li> <li> <p>Run TTS on Terminal</p> </li> <li>tts --text \"Text for TTS\" --out_path speech.wav</li> <li> <p>tts --text \"My name is sachin shetty\" --out_path speech.wav</p> </li> <li> <p>Docker Setup</p> </li> <li> <p>Reference</p> <ul> <li>https://github.com/coqui-ai/TTS</li> </ul> </li> </ul> <p>version: '3' services:   tts:     image: coqui/tts:latest     volumes:       - ./data:/root/.local/share/tts     command: tts-server --model_name tts_models/en/vctk/vits</p> <p>On host  machine \u2010 </p> <p>mkdir -p data/tts_models/en/vctk/vits wget -P data/tts_models/en/vctk/vits https://github.com/coqui-ai/TTS/releases/download/v0.11.0/vctk-vits.zip unzip data/tts_models/en/vctk/vits/vctk-vits.zip -d data/tts_models/en/vctk/vits</p> <p>https://www.perplexity.ai/search/Coqui-ai-tts-6DlPHfrkSHmq9QB3fE_cSg</p> <p>Building dockerfile </p> <p>FROM nvidia/cuda:11.3.1-cudnn8-devel-ubuntu20.04</p> <p>RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\     git \\     python3 \\     python3-pip \\     &amp;&amp; rm -rf /var/lib/apt/lists/*</p> <p>RUN pip3 install coqui-tts</p> <p>WORKDIR /app</p> <p>ENTRYPOINT [\"tts-server\"] CMD [\"--list_models\"]</p>"},{"location":"text-to-speech.html#docker-build-t-coqui-tts","title":"docker build -t coqui-tts .","text":""},{"location":"text-to-speech.html#docker-run-rm-it-p-50025002-coqui-tts-model_name-tts_modelsenvctkvits","title":"docker run --rm -it -p 5002:5002 coqui-tts --model_name tts_models/en/vctk/vits","text":"<p>-- With custom model - prebuilt </p> <p>FROM nvidia/cuda:11.3.1-cudnn8-devel-ubuntu20.04</p> <p>RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\     git \\     python3 \\     python3-pip \\     &amp;&amp; rm -rf /var/lib/apt/lists/*</p> <p>RUN pip3 install coqui-tts</p>"},{"location":"text-to-speech.html#download-and-extract-the-desired-models","title":"Download and extract the desired model(s)","text":"<p>RUN mkdir -p /root/.local/share/tts/tts_models/en/vctk/vits &amp;&amp; \\     wget -O - https://github.com/coqui-ai/TTS/releases/download/v0.11.0/vctk-vits.zip | bsdtar -xvf- -C /root/.local/share/tts/tts_models/en/vctk/vits</p> <p>WORKDIR /app</p> <p>ENTRYPOINT [\"tts-server\"] CMD [\"--model_name\", \"tts_models/en/vctk/vits\"]</p> <p>-- </p>"},{"location":"text-to-speech.html#cpu-only","title":"Cpu only","text":"<p>FROM python:3.8-slim</p> <p>RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\     git \\     &amp;&amp; rm -rf /var/lib/apt/lists/*</p> <p>RUN pip install coqui-tts</p> <p>WORKDIR /app</p> <p>ENTRYPOINT [\"tts-server\"] CMD [\"--list_models\"]</p> <p>-- docker build -t coqui-tts-cpu .</p> <p>-- docker run --rm -it -p 5002:5002 coqui-tts-cpu</p> <p>-- </p> <p>To optimize the Dockerfile for better performance in Coqui TTS, you can follow these steps:</p> <pre><code>Use a smaller base image\n</code></pre> <p>Instead of using the larger ubuntu:20.04 base image, consider using a more lightweight base image like python:3.8-slim or nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04. This can significantly reduce the overall image size.</p> <pre><code>Combine RUN commands\n</code></pre> <p>Combine multiple RUN commands into a single RUN command using &amp;&amp; to chain them together. This reduces the number of layers in the image, making it more efficient.</p> <p>dockerfile RUN apt-get update &amp;&amp; \\     apt-get install -y --no-install-recommends \\         git \\         python3 \\         python3-pip \\         &amp;&amp; \\     rm -rf /var/lib/apt/lists/*</p> <pre><code>Use multi-stage builds\n</code></pre> <p>Implement a multi-stage build to separate the build environment from the final runtime environment. This way, you can keep the final image lean by copying only the necessary files from the build stage.</p> <pre><code>Install dependencies from requirements file\n</code></pre> <p>Instead of installing coqui-tts directly, create a requirements.txt file with the required dependencies and install them using pip install -r requirements.txt. This allows better caching and faster builds when dependencies haven't changed.</p> <pre><code>Use Docker's built-in caching mechanism\n</code></pre> <p>Order the instructions in your Dockerfile such that the least frequently changing instructions are at the top. This way, Docker can reuse cached layers when possible, reducing build times.</p> <pre><code>Use NVIDIA's optimized Docker images\n</code></pre> <p>If you're running Coqui TTS on an NVIDIA GPU, use NVIDIA's optimized Docker images for machine learning, as they are designed for efficient model training and inference.</p> <pre><code>Leverage Docker volumes for data persistence\n</code></pre> <p>Instead of copying data into the Docker image, use Docker volumes to mount the necessary data (e.g., models, datasets) from the host machine. This keeps the image size small and allows for easy data management . By following these optimization techniques, you can significantly reduce the size of your Coqui TTS Docker image and improve its performance, making it more efficient for model training and inference.</p> <p>--</p> <p>Whisper with data</p> <p>version: '3' services:   whisper:     image: rhasspy/wyoming-whisper     command: --model base-int8 --language en     volumes:       - /path/to/host/model/data:/data     ports:       - 10300:10300</p>"},{"location":"triton-tensorRT-llm.html","title":"triton tensorRT llm","text":"<p>Triton Server Setup</p> <p>Build Triton     - git clone https://github.com/triton-inference-server/server     - cd server     - python build.py  </p> <p>Build Mixtral with Tensor RT-LLM</p> <pre><code>- git clone https://github.com/NVIDIA/TensorRT-LLM/\n- cd TensorRT-LLM\n- cd examples/mixtral\n- pip install -r requirements\n- git lfs install\n- git clone https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\n</code></pre> <ul> <li>Build Triton<ul> <li>https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/customization_guide/build.html</li> </ul> </li> <li> <p>Security</p> <ul> <li>https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/customization_guide/deploy.html</li> <li>https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/customization_guide/build.html#building-with-docker</li> </ul> </li> <li> <p>build mixtral</p> <ul> <li>https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/mixtral/README.md</li> </ul> </li> <li>https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html</li> <li>Build mistral - https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama#mistral-v01</li> </ul> <p>Extra  - sudo useradd -m llm - sudo passwd llm - sudo usermod -aG sudo llm  - apt install python3.10-venv  - apt install python3-pip  - sudo apt-get install build-essential linux-generic libmpich-dev libopenmpi-dev  - sudo apt install openmpi-devel</p>"},{"location":"tune-nodels.html","title":"Tune nodels","text":"<p>Fine tune models with Private Dataset</p> <ul> <li>Mistral Format - JSONL</li> <li> <p>{\"text\":\"[INST] Instruction[/INST] Model answer[INST] Follow-up instruction[/INST]\"}</p> </li> <li> <p>Install</p> <ul> <li>pip install langchain</li> </ul> </li> </ul> <p>Reference - https://github.com/SkunkworksAI/BakLLaVA - https://python.langchain.com/docs/integrations/llms/ollama - https://www.datacamp.com/tutorial/fine-tuning-llama-2 - https://apeatling.com/articles/part-2-building-your-training-data-for-fine-tuning/ - https://jsonlines.org/</p>"},{"location":"tutorials.html","title":"Tutorials","text":"<p>Tutorials</p> <p>Tutorials - v8     - Quantisation - tensors to GGUF         - llama.cpp     - Indic AI - Sarvam         - sarvam-ai - v7     - Mobile - On device Inference         - Android - v6     - GUI - Graphical User Interface        - UI : typescript + docker/ollama - v5     - Indoor Maps for Drone - v4     - Vision /Image Recognition         - vision-search.ipynb : moondream2/llava + ollama - v3      - Speech Output / TODO - v2     - Voice Output + v1         - speech-to-speech-inference.ipynb - Coqui tts + voice clone             - Speech to Speech Inference Demo - v1     - Voice Input + v0         - aws-bedrock-mistral-whisper.ipynb - Whisper + AWS Bedrock + Mistral + RestAPI - v0     - Rest API + local LLM         - local_function_call_rest_api.ipynb - function calling using mistral-inference and Mistral-7B-Instruct-v0.3         - ollama_mistral_function_calling.ipynb - function calling using ollama + mistral7b (4bit) + tokenizer.v3         - aws-bedrock-mistral.ipynbAWS Bedrock + Mistral Large + Function call</p>"},{"location":"visual-action.html","title":"Visual action","text":"<p>Visual Large Action Model</p> <ul> <li>Reference<ul> <li>https://github.com/openvla/openvla</li> <li>https://aiforspace.github.io/2024/</li> <li>https://github.com/TRI-ML/prismatic-vlms</li> <li>https://cvi2.uni.lu/spark2024/</li> <li>https://github.com/haotian-liu/LLaVA</li> <li>https://hliu.cc/</li> <li>https://github.com/google-deepmind/mujoco</li> <li>https://llava.hliu.cc/</li> <li>https://www.gradio.app/</li> </ul> </li> </ul>"},{"location":"vllm.html","title":"Vllm","text":"<p>Setup with Vllm</p> <ul> <li>Creat account in huggingface &gt; Profile &gt; AccessToken &gt; create new user Access token</li> </ul> <p>-- Docker Compose     - Compose     - Replace the args          - Hugging face Token         -  -- Docker Setup </p> <p>docker run --gpus all \\     -e HF_TOKEN=$HF_TOKEN -p 8000:8000 \\     ghcr.io/mistralai/mistral-src/vllm:latest \\     --host 0.0.0.0 \\     --model mistralai/Mistral-7B-Instruct-v0.2</p> <p>curl --location 'http://IP:Port/v1/chat/completions' \\ --header 'Content-Type: application/json' \\ --data '{         \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",         \"messages\": [             {\"role\": \"user\", \"content\": \"what minimun materials are necessary to build a Seed harvesting robot, show me how to arrange the parts\"}         ]     }'</p> <p>--</p> <p>References     - wsl + docker + nvidia runtime         - https://docs.nvidia.com/cuda/wsl-user-guide/index.html#running-cuda</p>"},{"location":"whisper.html","title":"Whisper","text":"<p>Whisper - Speect to text model</p> <ul> <li> <p>Docker - Start server</p> <ul> <li>docker run -p 5000:5000 slabstech/whisper-api TODO </li> <li>CPU<ul> <li>docker run -p 5000:5000 slabstech/whisper-api-cpu</li> </ul> </li> <li>GPU<ul> <li>docker run -p 5000:5000 slabstech/whisper-api-gpu</li> </ul> </li> </ul> </li> <li> <p>pyaudio required additional tools</p> <ul> <li>sudo apt-get install python3-dev portaudio19-dev</li> </ul> </li> <li> <p>Build Docker</p> <ul> <li>docker build --rm . -t slabstech/whisper-api-cpu -f Dockerfile</li> </ul> </li> <li>python python/whisper_api.py <ul> <li>Replace the path the audio file</li> </ul> </li> <li> <p>Inference     curl -F \"file=@/path/to/filename.mp3\" http://localhost:5000/whisper         - Ex : curl -F \"file=@/home/sachin/code/whisper/test1.flac\" http://localhost:5000/whisper {\"results\":[{\"filename\":\"file\",\"transcript\":\" What is your name? My name is Sachin.\"}]} </p> <ul> <li>You can test the API by sending a POST request to the route http://localhost:5000/whisper with a file in it. Body should be form-data.</li> </ul> </li> <li> <p>Run locally</p> </li> <li> <p>Clone Repo</p> </li> <li>git clone https://github.com/sachinsshetty/whisper.git</li> <li>Setup virtual environment</li> <li>python -m venv venv</li> <li>source venv/bin/activate</li> <li>Install whisper library<ul> <li>pip install -U openai-whisper</li> </ul> </li> <li>Install ffmpeg<ul> <li>sudo apt update &amp;&amp; sudo apt install ffmpeg</li> </ul> </li> <li>Optional<ul> <li>pip install setuptools-rust</li> </ul> </li> <li> <p>Run whisper</p> <ul> <li>whisper audio.flac audio.mp3 audio.wav --model medium</li> </ul> </li> <li> <p>Install sound recorder</p> <ul> <li>sudo apt install gnome-sound-recorder</li> </ul> </li> <li> <p>Model</p> <ul> <li>Small gives the nearest accurate answer with minimum resources.<ul> <li>small - 461 M</li> <li>tiny - 71 M</li> <li>base - 140 M</li> </ul> </li> </ul> </li> <li> <p>python example ` import whisper</p> </li> </ul> <p>model = whisper.load_model(\"small\") result = model.transcribe(\"audio.mp3\") print(result[\"text\"]) `</p> <ul> <li>References<ul> <li>https://huggingface.co/NexaAIDev/Octopus-v2</li> <li>moondream + octopus + phi-3</li> <li>https://github.com/snakers4/silero-vad/tree/master/examples/microphone_and_webRTC_integration</li> <li>https://hub.docker.com/r/onerahmet/openai-whisper-asr-webservice</li> <li>https://github.com/ventz/whisper-openai-container</li> <li>https://lablab.ai/t/whisper-api-flask-docker</li> <li>https://github.com/lablab-ai/whisper-api-flask</li> <li>https://github.com/meta-llama</li> </ul> </li> </ul>"},{"location":"wip.html","title":"Wip","text":"<p>WIP - Work in progress</p> <ul> <li> <p>Experiments</p> <ul> <li>GPT2 from scratch : llm.c </li> <li>Setup Raspi + ollama + mistral7B + RestAPi</li> <li>Prompt Optimization - DSPy + Mixtral + Ollama/Mistral API<ul> <li>Docs at dspy.md</li> <li>Code examples at dspy</li> </ul> </li> <li>Agents : autogen + vllm + gemma<ul> <li>VLLM setup </li> </ul> </li> <li>Agents : autogen + ollama + gemma<ul> <li>Setup + Documentation at agent-code.md </li> <li>Code examples at autogen</li> <li>Output from examples at agent-example-output.md</li> </ul> </li> <li>llama.cpp +  Phi model<ul> <li>Docs for setup of Phi model inference. </li> </ul> </li> </ul> </li> <li> <p>Mobile Interface - Android Multi-modal app search</p> </li> </ul>"},{"location":"2024/agent-code.html","title":"Agent code","text":"<p>simulation  Agents</p> <ul> <li> <p>Autogen </p> <ul> <li>https://github.com/microsoft/autogen</li> </ul> </li> <li> <p>Setup</p> <ul> <li>python3 -m venv pyautogenvenv<ul> <li>Linux -  source pyautogenvenv/bin/activate</li> <li>Windows - .\\pyautogenvenv\\Scripts\\activate</li> </ul> </li> <li>pip install pyautogen</li> <li>Docker setup - In a different directory<ul> <li>git clone --depth=1 https://github.com/microsoft/autogen</li> <li>docker build -f .devcontainer/Dockerfile -t autogen_base_img https://github.com/microsoft/autogen.git#main</li> </ul> </li> </ul> </li> <li> <p>Examples</p> <ul> <li>simple-agent</li> <li>docker-code-executor</li> <li>multi-agent-research</li> </ul> </li> <li> <p>Require individual endpoints for Agents to work efficiently.</p> </li> <li> <p>Create ollama containers and use it with docker compose. Build with hardcoded API endpoints initially and later optimise with load-balancer.</p> </li> <li> <p>Use smaller models to fit into GPU memory. Example </p> <ul> <li>24 GB card (RTX 4 series) can support 3 x- mistral 7B models</li> <li>12 GB card (RTX 3 series) can support 4 x - gemma2B model</li> <li>6 GB card (GTX 1060) can support 2 x - gemma2B model</li> </ul> </li> <li> <p>Run the different agent models in docker container</p> <ul> <li></li> </ul> </li> </ul> <p>1) Use a large model (Opus/GPT-4) to break down a problem &amp; generate prompts/variables/tasks/examples</p> <p>2) Use cheap (and way faster) models like Haiku to do the actual work</p> <p>KSP - </p> <p>https://wiki.kerbalspaceprogram.com/wiki/Tutorial:Understanding_Addon_Code</p> <p>https://wiki.kerbalspaceprogram.com/wiki/API:Part</p> <p>https://github.com/topics/ksp-mods</p> <p>https://github.com/zer0Kerbal</p> <p>https://wiki.kerbalspaceprogram.com/wiki/Setting_up_Visual_Studio</p> <p>https://wiki.kerbalspaceprogram.com/wiki/Tutorial:Creating_your_first_module</p>"},{"location":"2024/agent-example-output.html","title":"Agent example output","text":"<p>Example</p> <p>(pyautogenvenv) llm-recipes\\src\\autogen&gt; python.exe .\\simple-example.py Sure, here\u2019s a joke for you:</p> <p>What do you call an AI that\u2019s always giving you bad news?</p> <p>A bummer.</p> <p>(pyautogenvenv) llm-recipes\\src\\autogen&gt; python.exe .\\code-conversation.py code_executor_agent_docker (to code_writer_agent):</p> <p>Write Python code to calculate the 14th Fibonacci number.</p> <p>USING AUTO REPLY... code_writer_agent (to code_executor_agent_docker):</p> <pre><code># Calculate the 14th Fibonacci number using recursion.\ndef fibonacci(n):\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n\n\n# Print the 14th Fibonacci number.\nprint(fibonacci(14))\n</code></pre> <p>Provide feedback to code_writer_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:</p> <p>NO HUMAN INPUT RECEIVED.</p> <p>USING AUTO REPLY...</p> <p>EXECUTING CODE BLOCK (inferred language is python)... code_executor_agent_docker (to code_writer_agent):</p> <p>exitcode: 0 (execution succeeded) Code output: 377</p> <p>USING AUTO REPLY... code_writer_agent (to code_executor_agent_docker):</p> <p>Sure, here is the shell script to calculate the 14th Fibonacci number:</p> <pre><code>#!/bin/bash\n\n# Calculate the 14th Fibonacci number.\nfibonacci_num=$(fibonacci 14)\n\n# Print the 14th Fibonacci number.\necho \"$fibonacci_num\"\n</code></pre> <p>Provide feedback to code_writer_agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit</p> <p>(pyautogenvenv) llm-recipes\\src\\autogen&gt; python .\\two-agent-coding.py User (to Mistral Assistant):</p> <p>Count how many prime numbers from 1 to 10000.</p> <p>Mistral Assistant (to User):</p> <p>Shell Script</p> <pre><code>#!/bin/bash\n\n# Count the number of prime numbers from 1 to 10000\n\n# Initialize variables\ncount=0\nprime_count=0\n\n# Iterate through numbers from 1 to 10000\nfor num in $(seq 1 10000); do\n  # Check if the number is prime\n  if [[ \"$num\" -eq 1 -o \"$num\" -eq 0 ]]; then\n    # Increment the count of prime numbers\n    count=$((count + 1))\n  fi\ndone\n# Print the count of prime numbers\necho \"The count of prime numbers is $count\"\n</code></pre> <p>Explanation:</p> <ol> <li>The script uses a <code>for</code> loop to iterate through all the numbers from 1 to 10000.</li> <li>For each number, it checks if it is prime using the condition <code>if [[ \"$num\" -eq 1 -o \"$num\" -eq 0 ]]</code>.</li> <li>If it is prime, it increments the <code>prime_count</code> variable.</li> <li>After the loop has finished, it prints the final count of prime numbers.</li> </ol> <p>Execution:</p> <p>Save the script as <code>count_prime_numbers.sh</code> and make it executable using <code>chmod +x count_prime_numbers.sh</code>. Then, run the script by typing <code>./count_prime_numbers.sh</code> in the terminal.</p> <p>Output:</p> <p>The script will print the following output:</p> <pre><code>The count of prime numbers is 1023\n</code></pre> <p>Verification:</p> <p>Check the count of prime numbers in the output. It should match the value 1023, which is the total number of prime numbers from 1 to 10000.</p> <p>Provide feedback to Mistral Assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit</p> <p>(pyautogenvenv) llm-recipes\\src\\autogen&gt; python .\\two-agent-advanced.py UserProxy (to chat_manager):</p> <p>Write a Python function for the Fibonacci sequence, the function will have one parameter for the number in the sequence, which the function will return the Fibonacci number for.</p> <p>GroupChat select_speaker failed to resolve the next speaker's name. This is because the speaker selection OAI call returned: <pre><code>def fibonacci(n):\n  \"\"\"\n  Returns the nth Fibonacci number.\n\n  Args:\n    n: The index of the Fibonacci number to return.\n\n  Returns:\n    The nth Fibonacci number.\n  \"\"\"\n\n  # Base case for the Fibonacci sequence.\n  if n == 0:\n    return 0\n\n  # Return the Fibonacci number for the previous two indices.\n  else:\n    return fibonacci(n-1) + fibonacci(n-2)\n</code></pre> softwareCoder (to chat_manager):</p> <pre><code>def fibonacci(n):\n    \"\"\"\n    Calculates the nth Fibonacci number.\n\n    Args:\n        n (int): The index of the Fibonacci number to calculate.\n\n    Returns:\n        int: The nth Fibonacci number.\n    \"\"\"\n\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n</code></pre> <p>Example Usage:</p> <pre><code># Calculate the 10th Fibonacci number.\nfibonacci_number = fibonacci(10)\n\n# Print the result.\nprint(f\"The {fibonacci_number}th Fibonacci number is {fibonacci_number}\")\n</code></pre> <p>Output:</p> <pre><code>The 10th Fibonacci number is 55\n</code></pre> <p>Notes:</p> <ul> <li>The function uses recursion to calculate the Fibonacci numbers.</li> <li>The base cases for the Fibonacci sequence are 0 and 1.</li> <li>The function assumes the input <code>n</code> is a non-negative integer. If negative or non-integer inputs are allowed, the behavior may be unexpected.</li> </ul> <p>codeReviewer (to chat_manager):</p> <p>The code is well-written and provides a clear definition of the <code>fibonacci</code> function. It covers all the bases cases and follows the correct mathematical definition of the Fibonacci sequence.</p> <p>Suggestions for Improvement:</p> <ul> <li>Add error handling: Consider adding error handling for invalid input values, such as negative numbers or non-integer inputs.</li> <li>Use a more concise syntax: The code could be rewritten using a generator expression, which may be more efficient.</li> <li>Document the function: Adding comments or docstring would improve the code's readability and understanding.</li> </ul> <p>Revised Code with Improvements:</p> <pre><code>def fibonacci(n):\n    \"\"\"\n    Calculates the nth Fibonacci number.\n\n    Args:\n        n (int): The index of the Fibonacci number to calculate.\n\n    Returns:\n        int: The nth Fibonacci number.\n    \"\"\"\n\n    if n == 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        return fibonacci(n-1) + fibonacci(n-2)\n</code></pre> <p>With these improvements, the code will be more robust, easier to understand, and easier to maintain.</p> <p>softwareCoder (to chat_manager):</p> <p>The revised code is excellent and addresses the suggestions for improvement. It is well-written, concise, and efficient. Here's a breakdown of the changes:</p> <p>1. Error handling: * Added an <code>if</code> statement to handle invalid inputs, like negative numbers and non-integer values. * Returned <code>None</code> or provided an appropriate error message to indicate invalid input.</p> <p>2. Concise syntax: * Replaced the <code>else</code> block with a generator expression, which is more efficient for larger numbers.</p> <p>3. Documentation: * Added docstring explaining the function purpose and different base cases.</p> <p>4. Improved readability: * Removed unnecessary comments and used more meaningful variable names.</p> <p>5. Revised logic: * Combined base cases into a single <code>if</code> statement for clarity.</p> <p>Overall, the revised code is more robust, efficient, and easier to understand than the original version.</p> <p>codeReviewer (to chat_manager):</p> <p>The revised code is indeed much improved and provides a more robust and efficient solution to calculating Fibonacci numbers. The additional error handling, concise syntax, and improved documentation are all valuable additions that make the code more valuable.</p> <p>Here are a few additional notes:</p> <ul> <li>The function still assumes the input <code>n</code> is a non-negative integer. If negative or non-integer inputs are allowed, the behavior may not be as expected. Consider adding explicit validation or handling this case  explicitly.</li> <li>The generator expression approach may be less efficient for smaller values of <code>n</code> due to the overhead of creating and iterating over a generator object. However, it can be more efficient for larger numbers.    </li> <li>It would be possible to further optimize the code by using a memoization approach to cache the results of previously calculated Fibonacci numbers.</li> </ul> <p>Thank you for sharing your code and making improvements.</p> <p>softwareCoder (to chat_manager):</p> <p>The revised code is excellent and addresses the suggestions for improvement very effectively. It is well-written, concise, and efficient.</p> <p>Additional notes:</p> <ul> <li>The function still assumes the input <code>n</code> is a non-negative integer. If negative or non-integer inputs are allowed, the behavior may not be as expected. Consider adding explicit validation or handling this case  explicitly.</li> <li>The generator expression approach may be less efficient for smaller values of <code>n</code> due to the overhead of creating and iterating over a generator object. However, it can be more efficient for larger numbers.    </li> <li>It would be possible to further optimize the code by using a memoization approach to cache the results of previously calculated Fibonacci numbers.</li> </ul> <p>codeReviewer (to chat_manager):</p> <p>The revised code is indeed very well improved and provides a much more robust and efficient solution to calculating Fibonacci numbers. The additional features, such as error handling, improved code readability, and optimization potential, make it an even more valuable addition to the original code.</p> <p>Specific suggestions for further improvement:</p> <ul> <li>Refactoring: Consider refactoring the code to remove redundant code or improve its structure.</li> <li>Unit tests: Adding unit tests can help ensure the code is working as expected and provides a clear pass/fail indication.</li> <li>Documentation: Improve the documentation to provide clear instructions and examples for usage.</li> <li>Alternative implementation: Explore alternative approaches, such as using a mathematical formula or memoization, to compare efficiency.</li> </ul> <p>softwareCoder (to chat_manager):</p>"},{"location":"2024/agent-example-output.html#further-improvements-to-the-revised-code","title":"Further Improvements to the Revised Code","text":"<p>Based on the additional notes and suggestions, here are some potential improvements to the revised code:</p> <p>1. Refactoring:</p> <ul> <li>Combine related functions into a single function to reduce code duplication.</li> <li>Use a function decorator to enhance the readability and maintainability of the code.</li> <li>Use context managers for resource management, especially when dealing with file operations.</li> </ul> <p>2. Unit tests:</p> <ul> <li>Write unit tests to ensure the code is working as expected and covers different edge cases.</li> <li>Utilize a mocking library to mock dependencies and simplify unit testing.</li> <li>Add comprehensive test cases for edge cases and invalid inputs.</li> </ul> <p>3. Documentation:</p> <ul> <li>Provide clear and concise instructions on how to use the function, including examples.</li> <li>Include a usage example demonstrating the correct syntax and output.</li> <li>Add a section about optimization techniques, including the use of memoization.</li> </ul> <p>4. Alternative implementation:</p> <ul> <li>Consider using a mathematical formula for calculating Fibonacci numbers to improve performance for large <code>n</code>.</li> <li>Explore using a memoization approach to store the results of previously calculated Fibonacci numbers.</li> <li>Utilize dynamic programming techniques for efficient computation of Fibonacci numbers for specific cases.</li> </ul> <p>By implementing these improvements, you can further enhance the code's quality, maintainability, and performance.</p> <p>codeReviewer (to chat_manager):</p> <p>The revised code is an excellent improvement over the original version. The suggestions for improvement provide valuable insights and highlight potential areas for further optimization. Here are some additional improvements that could be considered:</p> <p>1. Benchmarking:</p> <ul> <li>Measure the execution time of the code with different inputs to evaluate its performance.</li> <li>Identify areas where performance can be improved and focus on optimizing those parts.</li> </ul> <p>2. Error handling:</p> <ul> <li>Consider adding additional error handling cases, such as division by zero or invalid input values.</li> <li>Provide specific error messages and return values to indicate the type of error.</li> </ul> <p>3. Unit test coverage:</p> <ul> <li>Ensure that unit tests cover a wide range of scenarios, including different input values and corner cases.</li> <li>Use a diverse set of test cases to validate the functionality and edge cases.</li> </ul> <p>4. Code readability improvements:</p> <ul> <li>Use consistent variable names and code formatting for improved readability.</li> <li>Add comments to explain complex sections of the code.</li> </ul> <p>5. Optimization opportunities:</p> <ul> <li>Explore using memoization or dynamic programming for large <code>n</code> to improve performance.</li> <li>Utilize efficient data structures and algorithms for calculating Fibonacci numbers.</li> </ul> <p>By implementing these additional improvements, you can further enhance the code's quality and performance.</p> <p>softwareCoder (to chat_manager):</p> <p>Additional Suggestions for Improvement:</p> <ul> <li>Consider using a linter to enforce code quality and identify potential errors.</li> <li>Write unit tests for the revised code to ensure its functionality and performance.</li> <li>Provide a comprehensive documentation that covers installation, usage, and best practices.</li> <li>Refactor the code to improve its readability and maintainability.</li> <li>Benchmark the code to evaluate its performance for different inputs.</li> <li>Consider using a version control system to manage code changes and track improvements.</li> </ul> <p>codeReviewer (to chat_manager):</p> <p>The revised code is a significant improvement over the original version. The suggestions for improvement provide valuable insights and highlight areas for further optimization. The additional improvements you mentioned are all excellent suggestions and should be considered to further enhance the code's quality and performance.</p> <p>Specific Suggestions for Improvement:</p> <ul> <li>Benchmarking: Measure the execution time of the revised code with different inputs to evaluate its performance.</li> <li>Error handling: Consider using a more robust error handling mechanism, including handling exceptions and invalid input types.</li> <li>Code documentation: Add detailed comments and docstring to enhance code readability and understanding.</li> <li>Optimize performance: Explore using memoization, dynamic programming, or other techniques to optimize performance for large <code>n</code>.</li> <li>Unit test coverage: Ensure that unit tests cover a wide range of scenarios, including different input values and corner cases.</li> <li>Version control: Utilize a version control system to manage code changes and track improvements.</li> </ul> <p>GroupChat select_speaker failed to resolve the next speaker's name. This is because the speaker selection OAI call returned: SoftwareCoder</p> <p>I am the software coder, and I would be happy to help further improve the revised code.</p> <p>How can I assist you in improving the code?</p> <p>USING AUTO REPLY... UserProxy (to chat_manager):</p>"},{"location":"2024/arc-prize.html","title":"Arc prize","text":"<p>Arc PRize</p> <p>https://www.kaggle.com/competitions/arc-prize-2024/overview</p> <p>https://github.com/fchollet/ARC-AGI</p> <p>https://github.com/neoneye/ARC-Interactive-History-Dataset</p> <p>https://braingridgame.com/</p>"},{"location":"2024/august-2024-todo.html","title":"August 2024 todo","text":"<p>Work on vision problem </p> <p>Solve the deployment feature using Tello drone</p> <p>Only start a new module, when the current work is fully complete. Dont jump into new implementation with the prior work half done.</p> <p>Improve the documentation for all the work done till now,  Make it easily readable.</p> <p>Features 1. Installation for cuda, docker, ollama,  2. Download models for vision , speech, text language models for ollama. 3. Speech to Speech - integration  4.  Download models via huggingface for Vllm, llama.cpp, fine tuning  5.  Model quantisation with llama.cpp 6.  Spaces deployment using huggingface.co  7.  Video tracking using Segment Anything and grounding dino </p>"},{"location":"2024/automata.html","title":"Automata","text":"<p>Design of an Automata</p> <p>Find the - requirements </p> <p>Build the first working prototype with zero extra feature </p> <p>It should just work</p> <p>Build the next prototype with 1 extra feature </p> <p>Build the next prototype with 1 extra feature with more reliability in software. </p> <p>After each prototype,  request input from the user.</p> <p>To continue building  Or to stop execution. </p> <p>At the first step, build a planner.</p> <p>Show the user, what will be achieved in iterative steps.</p> <p>Keep everything in a sandbox. </p> <p>All experiments are done offline. We only have access to models of different levles. Running locally.</p>"},{"location":"2024/aws-genai.html","title":"Aws genai","text":"<p>AWS GenAi workshop - June 5</p> <p>Code Samples - https://github.com/aws-samples/amazon-bedrock-workshop</p> <p>Amazon Bedrock 1. Curated dataset 2. Evaluation 3. Reviewers 4. Customer metric 5. Get Results</p> <p>Customzing Foundation models</p> <ol> <li>Prompt Engineering</li> <li>Retrieval Augemented Generation (RAG)</li> <li>Fine tuning</li> <li>Continued Pre Training</li> </ol> <p>RAG = Retrieval Augemented Generation</p> <p>Vector Embeddings</p> <p>Source Data -&gt; Tokenization -&gt; Vectorization -&gt; Store in vector data store -&gt; Perform semantic similarity search -&gt; Include semantically similar context in prompt</p> <p>How to build Knowledge bases - Automatic</p> <p>Vector Database - Enabling semantic search</p> <p>Agents for Amazon Bedrock     - Chain of thought/ chain executiion Enable GenAI to execute multi step tasks using ompany systems and data sources</p> <p>Guardrails for Amazon Bedrock - content moderation as bedrock  - </p> <p>Inference Consumption options - On demand  - Provisioned throughput </p> <p>Batch mode / load large dataset</p> <p>Bedrock Studio / Sagemaker studio</p> <p>Organizer  lilzheng AT amazon DOT de</p> <p>--</p> <p>https://aws.amazon.com/blogs/aws/tackle-complex-reasoning-tasks-with-mistral-large-now-available-on-amazon-bedrock/</p> <ul> <li>postgresl + pg_vector extension : RAG Document extractor : </li> </ul> <p>==</p> <p>Go to Model access and enable the required mistral model https://us-west-2.console.aws.amazon.com/bedrock/home?region=us-west-2#/modelaccess</p> <p>Get AWS CLI credentials</p> <p>copy the parameters</p> <p>export AWS_DEFAULT_REGION=\"us-west-2\" export AWS_ACCESS_KEY_ID=\"KEYIDASDASD\" export AWS_SECRET_ACCESS_KEY=\"SECREATEACESASDASKEY\" export AWS_SESSION_TOKEN=\"sesioandateoadlkalsda\"</p> <p>add to bash and execute the notebook</p> <p>--</p>"},{"location":"2024/deployment-speech-demo.html","title":"Deployment speech demo","text":"<p>Deployment </p> <p>Single container - Dockerfiles 1. Cpu only 2. Gpu only </p> <p>Monolithic build  Call internal libs directly </p> <p>Full control of all components,  Optimized for usability, </p> <p>Do performance optimization later after demo</p> <p>Build  1.whisper  2. Coqui 3. Ollama  4. Ffmpeg </p> <p>Download mistrak7b, whisper base , coqui - tachtron, voice clone </p> <p>Multi stage docker build</p> <p>Build individual components and finally copy only build libraries to deploy image </p> <p>Use cpu and gpu separately  -  Image name - speech-to-speech-cpu</p> <p>first cpu only, push to docker hub for docker compose .</p> <p>Multi container - Modify existing compose files for multi container support with gpu. Use the prebuilt binaries for gpu. Added additonal data after download and rebuild docker file.  Save a checkpoint for immediate use.</p>"},{"location":"2024/hackathon-may-2024.html","title":"Hackathon may 2024","text":"<p>Hackathon - May 25-26, 2024</p> <p>Replicate Figure 01 Robot - Speech to Speech Reasoning All component running locally with OpenSource tools and OpenWeights models</p> <p>Live Blog at X - https://x.com/gaganyatri/status/1794121114374828301</p> <p>Hackathon Status - Real-time data with locall LLM- mistral7b + function_calling code - local_function_call_rest_api </p> <p>In Scope -  1. Scene/Image description 2. Voice to Text  - conversion 3. Text Q &amp; A with Live Data Fetch 4. Text to Voice</p> <p>Out of Scope - Humanoid Robot</p> <p>Status</p> <p>What is currently in progress ? - Real Time info interface to Mistral7B model</p> <p>What is completed ?</p> <p>What needs to be done ?</p> <p>llm-recipes - path to Jarvis </p> <p>What does the repo contain ?  Steps with code to run a llm model locally with below capability  - ChatBot UI - openweb-ui + ollama + docker </p> <p>WIP  - RealTime data- mistral7b + function_calling interface + vllm </p> <p>Future </p> <ol> <li>Voice to text - whisper ??</li> <li>Text to voice - eleven labs ??</li> <li>Image to text - llava </li> <li>Text to image - stable diffusion </li> <li>Gpt assistant- RAG ? / fune tuning </li> </ol> <p>Constraints -</p> <ol> <li>System requirements  - Minimum 20GB vram for inference, mistral 7B is 16GB at full precision ,  query  32k context window x 16bits = ?? </li> </ol> <p>Solution - quantize to 4bits = 4 GB +</p> <ol> <li> <p>Deployment  Vllm deployment with docker  Need to experiment to find clean docker compose </p> </li> <li> <p>Real time - browsing Capacity // perplexity clone ?? Create function with Rest api endpoint detection  Summarise website info into required format. </p> </li> </ol>"},{"location":"2024/kletech-workshop.html","title":"Kletech workshop","text":"<p>Workshop - LLM Usage with Local</p> Why Learn CS Fundamentals  - **Database:**     - Django Models: Understand how to define and use models in Django to structure your data.     - Application Logic: Learn how to implement business logic using Django's ORM.     - Data Travel: Understand the flow of data within your application. - **Network:**     - Docker: Learn about containerization and how Docker can help manage your application's environment.     - Data Travel: Understand where and how data travels within your network. - **Algorithm:**     - Prompt Optimization: Learn how to optimize prompts for better results with LLMs.     - Scalability: Understand how to design algorithms that can handle large amounts of data.     - Domain Specific: Learn how to use LLMs effectively in specific domains like text, speech, and vision.  Kletech Workshop  ## Cool Demos - [AI.town](https://ai.town) - Talk to the riddler - [Gaganyatri.in](http://gaganyatri.in) - AI app deployed with Zero cost and Open source   LLM - Large Language Models - SOTA  - **Text**: Mistral, Meta's llama3.1 - **Vision**: Pixtral, Moondream, llava - **Speech**: Whisper, Ultravox, Moshi   Research Papers to Read  - \"Transformer: Attention is all you need\" by Vaswani et.al - \"Llama3.1: Project Release Notes\"   Prerequisites  - Ubuntu 22/24 - Docker - VSCode - cuda-toolkit - nvidia GPU drivers   Top Research - Open Source  - **Mistral**: Text - mistral-nemo, Vision - Pixtral - **Meta**: llama 3/3.1, SAM2 - **OpenAI**: whisper, tiktoken - **Indian**: sarvam.ai, ai4bharat   <ul> <li>Demo</li> <li>gaganyatri.in </li> <li>text</li> <li>vision</li> <li>json - analysis </li> <li>food guardian </li> <li> <p>ai.town riddler </p> </li> <li> <p>Video - stream to YouTube </p> </li> <li>Drone - product - dji neo- follow me</li> <li>todo - djintello - track person- object detection</li> </ul> <p>llm - open source projects - openweb-ui : chatgpt ui - ollama : model engine  - vscode + continue: dev co pilot  - vlmm serving : h100 todo  - gaganyatri.in : explain in detail </p> <ul> <li>edge ai - deploy small model locally,  mobile via executorch for Llama 3.2 1b </li> <li>mobile app - cloud api, on device model </li> </ul>"},{"location":"2024/mannheim-hackathon.html","title":"Mannheim hackathon","text":"<p>Mannheim hackathon- 6-8 sep</p> <p>Project- waste reduction at Super market</p> <p>Version 1 - Image  warehouse  - local  - Huggingface.co </p> <p>Version 2 - video - local  - huggingface</p> <p>Version 3 - segment anything - local  - Huggingface.co </p> <p>6 - friday  Event start at 2 .</p> <p>Leave from wurzburg at 11  Train travel - 3 hours </p> <p>Sleep with bag at 12 </p> <p>7 - Saturday </p> <p>Hotel check in at 12 </p> <p>Come back to event at 3</p> <p>8 - Sunday </p> <p>Event complete at 3pm</p> <p>Leave mannheim at 5pm .</p> <p>Luggage  Sleeping bag  Small blue trolley  Charged Drone  Laptop and charger  Laptop lock  Grooming kit  Tablet - charged to read </p>"},{"location":"2024/mars-multi-agents.html","title":"Mars multi agents","text":"<p>Mars multi agent systems</p> <p>Mars will require everything that has been invented till now and many more things that need to be invented to survive 1 day for Human civilization. </p> <p>Model the systems based on Autonomous agents  which are fine-tuned for specific tasks on LLMs. Agents should solve problems identified during work process and fix the systems. </p>"},{"location":"2024/multi-billion-agents-india.html","title":"Multi billion agents india","text":"<p>Multi-(Billion)Agent system - Designs of developing India</p> <p>Design scaling law for agents- What models and inference speed is required for agents to work efficiently. No. Of Agents vs No. Of servers vs No. Of models </p> <p>Model  - energy consumption  - logistics planning - disaster avoidance,  recovery and planing </p> <p>Use data from data.gov.in </p> <p>Build a picture of India,  what changes in policies can change the demography of Nation.</p> <p>Scale up from Self - family - locality- city - district- state - country </p>"},{"location":"2024/on-device.html","title":"On device","text":"<p>On Device - Code </p> <p>Refactoring has been automated via LLM</p> <p>Code is now available to anyone with curiosity</p> <p>Build the mobile app to replace Google translate. Use embedding model</p> <p>Use mistral7b as server with phone. Root device to run ollama</p> <p>Mobile App     - v1          - use local API for query         - Build simple interace for Image/Voice/Text - Chat App     - v2         - Move around the models till latency is reduced</p>"},{"location":"2024/research-plan.html","title":"Research plan","text":"<p>Admin work  Change localgpt gpu to device 3/ Gpu 4</p> <p>Put notification of gpu server bash on current access </p> <p>Create a gpu server user group in mattermost</p> <p>Learn at the edge  Use discord server to learn new examples </p> <p>Work plan </p> <p>Build a load balancer for gpu 3/ device 2 with docker compose for agent research.  Create a health check for container to load required model on startup  Mistral for planner with 33% ram, gemma2b for other agents with 22 % ram x 3 containers.</p> <p>Serve requests to reasoning agents with a proxy .</p> <p>How is caching used with Autogen to reduce network round trip.</p> <p>Problem solving- Reasoning agent </p> <p>Use a function calling model to solve leet code. Multi agent - Group chat for leet code.</p> <p>Work - integration </p> <p>Agent with function calling - connect to operations api and display information </p>"},{"location":"2024/system.html","title":"System","text":"<p>System - PC build</p> <p>Power requirements     - Set Power rating for GPU          - with ollama - A5000, A2000</p> <p>TinyBox      - Red - 7900XTX     - Green - 4090 </p> <ul> <li>Reference <ul> <li>https://www.tomshardware.com/reviews/evga-supernova-1600-t2-psu,5414.html</li> <li>https://www.silverstonetek.com/en/product/info/power-supplies/HELA2050Platinum/</li> <li>https://docs.runpod.io/hosting/burn-testing </li> <li>https://docs.runpod.io/tutorials/pods/run-your-first</li> <li>https://github.com/divamgupta/diffusionbee-stable-diffusion-ui</li> </ul> </li> </ul>"},{"location":"2024/teleoperations.html","title":"Teleoperations","text":"<p>Teleoperations </p> <p>Use the existing code,  build something .</p> <p>Create version with paper board. </p> <p>Make 3d print for second version</p> <p>Build the humanoid part for Habitat, Handle movemnts of v items to setup laboratory </p> <p>Robots for Habitat </p> <p>1 Mobile uav  2 Humanoid  3 Mobile ugv</p> <p>Some progress is done with Uav drone.</p> <p>Next step is Humanoid or Robot arms </p>"},{"location":"2024/valorant-hackathon.html","title":"Valorant hackathon","text":"<p>Valorant + AWS - Hackathon</p> <p>What to Build</p> <p>You are hired as a data scientist on a new VALORANT esports team and have been tasked by the team\u2019s general manager to support the scouting and recruitment process.</p> <p>Build a LLM-powered digital assistant with a chat interface using Amazon Bedrock\u2019s native capabilities. Use this new technology to build teams and answer various questions about VALORANT esports players, leveraging provided data sources and demonstrating effective information retrieval and analysis.</p> <p>LLM-Powered Digital Assistant Requirements</p> <p>The digital assistant should be able to provide team compositions based on the following prompts, assign player roles, and justify the team effectiveness:</p> <pre><code>Professional Team Submission: Build a team with pro players (VCT International) only\n\nSemi-Professional Team Submission: Build a team with semi-pro (VCT Challengers) players only\n\nVCT Game Changers Team Submission: Build a team with VCT Game Changers players only\n\nMixed-Gender Team Submission: Build a team with at least 2 players from an underrepresented group (ex. Game Changers)\n\nCross-Regional Team Submission: Build a team with players from 3+ regions\nRising Star Team Submission: Build a team that includes at least two semi-professional players (VCT Challengers or VCT Game Changers)\n</code></pre> <p>For each team composition:</p> <pre><code>Answer questions about player performance with specific agents (in-game playable characters)\nAssign roles to players on the team and explain their contribution\n    Offensive vs. defensive roles\n    Category of in-game playable character / agent (duelist, sentinel, controller, initiator)\n    Assign a team IGL (team leader, primary strategist and shotcaller)\nProvide insights on team strategy and hypothesize team strengths and weaknesses\n</code></pre> <p>Prompts</p> <p>Your digital assistant should be able to provide responses to all of the following prompts. The system should also provide its reasoning for each prompt in the results:</p> <pre><code>Team Submission Prompts - Main Prompts\n\n    Professional Team Submission: \"Build a team using only players from VCT International. Assign roles to each player and explain why this composition would be effective in a competitive match.\"\n\n    Semi-Professional Team Submission: \"Build a team using only players from VCT Challengers. Assign roles to each player and explain why this composition would be effective in a competitive match.\"\n\n    Game Changers Team Submission: \"Build a team using only players from VCT Game Changers. Assign roles to each player and explain why this composition would be effective in a competitive match.\"\n\n    Mixed-Gender Team Submission: \"Build a team that includes at least two players from an underrepresented group, such as the Game Changers program. Define roles and discuss the advantages of this inclusive team structure.\"\n\n        To be used for bonus prizing evaluation\n\n    Cross-Regional Team Submission: \"Build a team with players from at least three different regions. Assign each player a role and explain the benefits of this diverse composition.\"\n\n        To be used for bonus prizing evaluation\n\n    Rising Star Team Submission: \"Build a team that includes at least two semi-professional players, such as from VCT Challengers or VCT Game Changers. Define roles and discuss details of how these players were chosen.\"\n\n        To be used for bonus prizing evaluation\n</code></pre> <p>Finalists will receive additional follow-up prompts after the down selection process. Please ensure your digital assistant can elaborate on the team compositions and provide reasoning for questions such as \"What recent performances or statistics justify the inclusion of player name in the team?\"</p> <p>Stage Two Judging information:</p> <p>At least 10 projects, but no more than 25 projects will be selected to complete Finalist Testing from October 30, 2024 (1pm Pacific Time) to November 5, 2024 (1pm Pacific Time). During the Finalist Testing stage, we will send a few additional prompts for finalists to test.</p> <p>Finalists may make minor adjustments to their models in response to the prompts, however, no major changes will be allowed. The modified project must remain substantively the same as the original project with the only modification being what we permit.</p> <p>https://vcthackathon.devpost.com/details/submissionspecifics</p>"},{"location":"2025/2024-02-case-study.html","title":"2024 02 case study","text":"<p>Case Study - Google AI</p> <ul> <li>Challenge</li> <li>Build an Immersive Audiobook with GenAI</li> </ul> <p>1. Direct prompt pdf without separater parser, No chunks and cleanup. </p> <ol> <li> <p>Combine all prompts into single prompt for full scene info and dialogs</p> </li> <li> <p>Use Google tts with fixed voice and batched prompts</p> </li> </ol> <p>4  Combine prior output for Audio. </p> <p>Compsre total lines of code and deployment costs avoided. </p>"},{"location":"2025/2025-02-17-shopping-bot.html","title":"2025 02 17 shopping bot","text":"<p>Shopping Bot </p> <p>Fast Prototype :  Build iterative solutions - </p> <p>Write - requirements </p> <p>Find existing solutions : Understand main parts</p> <p>Chrome dev tools analysis </p> <p>Browser agent : omni parser- microsoft </p> <p>Moondream - computer use </p> <p>DeepResearch - with credentials for work</p> <p>--</p> <p>Event based agent triggers. </p> <p>New entry into NoSql db, triggers a serverless call for inference based on prompt and </p> <p>connect to existing systems via API </p> <p>--</p> <p>Integrate Agent solutions for generating and using data </p>"},{"location":"2025/2025-02-17-shopping-bot.html#-","title":"-","text":"<p>examples -  stripe finance agents  perplexity shopping  deep research : tool building</p> <p>--</p> <p>hackathon - workshop </p> <p>Mission Primer</p> <p>Drone tactics </p>"},{"location":"2025/2025-02-17-shopping-bot.html#on-the-go-real-time-monitoring","title":"On the go, real time monitoring","text":"<p>Currently no - efficient systems. </p> <p>Edge compute: using meshed netwirks</p> <p>Sending data in all wavelngths </p> <p>Add listeners on edge devices, to pick up any signals </p> <p>Build intelligence for the deep echelon missions.</p> <p>Systems the are cut off,  Drop meshed system to pick sensors infi and send collected data. </p> <p>Use drones to create meshed computer's. </p> <p>Eyeson : workshop</p> <p>Camera/video understanding via fast cv method like yolo. Not transformer based for scene understanding. </p> <p>Ask about hardware setup. </p> <p>Gemini - multimodal Api -  audiobook </p> <p>1. Direct prompt pdf without separater parser, No chunks and cleanup. </p> <ol> <li> <p>Combine all prompts into single prompt for full scene info and dialogs</p> </li> <li> <p>Use Google tts with fixed voice and batched prompts</p> </li> </ol> <p>4  Combine prior output for Audio. </p> <p>Compsre total lines of code and deployment costs avoided. </p>"},{"location":"2025/2025-hackathons.html","title":"2025 hackathons","text":"<p>Hackathon - 2025</p> <ul> <li>March 29, 2025, 24 H - Lovable Hackathon</li> <li>https://launched.lovable.dev/submit</li> <li>https://hackathon-anthropic.lovable.app/</li> <li> <p>https://lu.ma/r70gxc42?tk=s0jynl</p> </li> <li> <p>March 29-30, 2025, EDTH - Amsterdam</p> </li> <li></li> </ul>"},{"location":"2025/ffmpeg-commands.html","title":"Ffmpeg commands","text":"<p>ffmpeg commands</p> <ul> <li> <p>reducs fps to lower memory</p> <ul> <li>ffmpeg -i input.mp4 -r 15 -c:v libx264 -c:a aac output.mp4</li> </ul> </li> <li> <p>Video  - webm to mp4 </p> <p>Prepare Your Files: Place your two WEBM files (let\u2019s call them video1.webm and video2.webm) in the same folder. Create a Text File: Make a file called filelist.txt in the same folder and add these lines:</p> <p>file 'video1.webm' file 'video2.webm'</p> <p>Run the Command: Open a terminal or command prompt, navigate to the folder with your files, and run:</p> <p>ffmpeg -f concat -i filelist.txt -c:v libx264 -c:a aac output.mp4</p> <p>This concatenates the videos and converts them to MP4 with H.264 video and AAC audio, which is YouTube-friendly. Check the Output: You\u2019ll get a file named output.mp4 ready for upload.</p> </li> </ul> <ul> <li> <p>Video - webm to mp4  - With deduplication</p> <p>Place Your Files: Put your two WEBM files (e.g., video1.webm and video2.webm) in one folder. Remove Duplicates Within Each Video:     Run this command for each file to filter out duplicate frames:</p> <pre><code>ffmpeg -i video1.webm -vf mpdecimate -c:v libx264 -c:a aac video1_clean.mp4\nffmpeg -i video2.webm -vf mpdecimate -c:v libx264 -c:a aac video2_clean.mp4\n\nThe mpdecimate filter detects and drops frames that are nearly identical to the previous one, effectively removing duplicates. The output will be video1_clean.mp4 and video2_clean.mp4.\n</code></pre> <p>Combine the Cleaned Videos:     Create a filelist.txt file in the same folder with:</p> <pre><code>file 'video1_clean.mp4'\nfile 'video2_clean.mp4'\n\nConcatenate them into a single MP4:\n\nffmpeg -f concat -i filelist.txt -c:v libx264 -c:a aac output.mp4\n</code></pre> <p>Verify and Upload: Check output.mp4 to ensure it looks good, then upload it to YouTube.</p> </li> </ul>"},{"location":"2025/tldr.html","title":"Tldr","text":"<p>Articles to read</p> <ul> <li> <p>The Ultra-Scale Playbook: Training LLMs on GPU Clusters -  https://huggingface.co/spaces/nanotron/ultrascale-playbook</p> </li> <li> <p>https://jax-ml.github.io/scaling-book/</p> </li> <li> <p>CUDA AI Engineer - Sakana AI - https://pub.sakana.ai/static/paper.pdf</p> <ul> <li>https://pub.sakana.ai/ai-cuda-engineer/</li> </ul> </li> <li> <p>https://harper.blog/2025/02/16/my-llm-codegen-workflow-atm/</p> </li> </ul>"},{"location":"papers/vlm.html","title":"Vlm","text":"<p>Vision Language Models</p> <p>VLM paper -1 https://huggingface.co/collections/merve/mit-talk-31-10-papers-671f6a16e156f77739820c89</p> <p>Vision Language Models 2-  https://huggingface.co/collections/merve/vision-language-models-papers-66264531f7152ac0ec80ceca</p> <p>Image to Text - https://huggingface.co/collections/merve/image-to-text-models-650a005f5ac587b3f37de934</p> <p>Text to Image - https://huggingface.co/collections/merve/text-to-image-models-650a016f2257a3afbaef627e</p> <p>Image to Image - https://huggingface.co/collections/merve/image-to-image-models-6509ebf3948ce5dce8be527e</p>"},{"location":"research/gpu_mode.html","title":"Gpu mode","text":"<p>GPU Mode</p> <p>https://www.youtube.com/watch?v=FH5wiwOyPX4</p> <p>https://www.youtube.com/playlist?list=PLV3CP0t7OVdq-FwBTwn2Vm0-JgRR3fbko</p> <ul> <li>Paper -<ul> <li>TPI-LLM: SERVING 70B-SCALE LLMS EFFICIENTLY ON LOW-RESOURCE EDGE DEVICES</li> <li>https://arxiv.org/pdf/2410.00531</li> </ul> </li> </ul>"}]}